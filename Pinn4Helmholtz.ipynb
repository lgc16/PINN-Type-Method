{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as v\n",
    "\n",
    "import Net_and_Loss as nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.6.0+cu101\n"
     ]
    }
   ],
   "source": [
    "is_gpu = torch.cuda.is_available()\n",
    "if is_gpu:\n",
    "    id = 1\n",
    "    torch.cuda.set_device(id)\n",
    "    \n",
    "#gpu_nums = torch.cuda.device_count()\n",
    "#gpu_index = torch.cuda.current_device()\n",
    "#print(is_gpu,gpu_nums,gpu_index)\n",
    "device = torch.device('cuda' if is_gpu else 'cpu')\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "print(device)\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "k , theta0 = 4 , np.pi/7\n",
    "k1,k2 = k*np.cos(theta0) , k*np.sin(theta0) \n",
    "sample_num = 800 \n",
    "k_2 = k*k\n",
    "theta = torch.linspace(2*np.pi/sample_num,2*np.pi,sample_num).reshape(-1,1)\n",
    "r = 9/20-1/9*torch.cos(5*theta)\n",
    "x0,x1 = torch.cos(theta),torch.sin(theta)\n",
    "sample_x = v(torch.cat((x0,x1),1)*r,requires_grad=True)\n",
    "\n",
    "sample_u_r = torch.cos(k1*sample_x[:,0]+k2*sample_x[:,1]); \n",
    "sample_u_i = torch.sin(k1*sample_x[:,0]+k2*sample_x[:,1]);\n",
    "\n",
    "sample = 2400\n",
    "theta_in = torch.rand(sample,1)*2*np.pi\n",
    "r_in = torch.rand(sample,1)*(9/20-1/9*torch.cos(5*theta_in))*0.999\n",
    "x0_in = torch.cos(theta_in)\n",
    "x1_in = torch.sin(theta_in)\n",
    "x_in = v(torch.cat((x0_in,x1_in),1)*r_in,requires_grad = True)\n",
    "\n",
    "class myNet1(nn.Module):\n",
    "  def __init__(self,m,out):\n",
    "    super().__init__()\n",
    "\n",
    "    self.net=nn.Sequential(\n",
    "      nn.Linear(in_features=2,out_features=m),nn.Sigmoid(),\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\n",
    "      nn.Linear(m,m),nn.Sigmoid(),\n",
    "      nn.Linear(m,2)\n",
    "    )\n",
    "  def forward(self, input:torch.FloatTensor):\n",
    "    return self.net(input)\n",
    "class Pinn_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,ddu_r,ddu_i,u_r,u_i,bd_out_r,bd_out_i,bd_u_r,bd_u_i,k2):\n",
    "        return torch.mean((ddu_r+k2*u_r)**2+(ddu_i+k2*u_i)**2)+1*torch.mean((bd_out_r-bd_u_r)**2+(bd_out_i-bd_u_i)**2)\n",
    "    \n",
    "mynet = myNet1(40,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet = mynet.to(device)\n",
    "x_in = x_in.to(device)\n",
    "sample_x = sample_x.to(device)\n",
    "sample_u_r,sample_u_i = sample_u_r.to(device),sample_u_i.to(device) \n",
    "out_r = mynet(x_in)[:,0]\n",
    "c = (torch.ones(out_r.size())).to(device)\n",
    "out_r,out_i = mynet(x_in)[:,0],mynet(x_in)[:,1]\n",
    "bd_out_r,bd_out_i = mynet(sample_x)[:,0],mynet(sample_x)[:,1]\n",
    "dx_r = torch.autograd.grad(out_r, x_in, grad_outputs=c,retain_graph=True,create_graph=True)[0]\n",
    "grad_x_r = (dx_r[:,0]).reshape(-1,1)\n",
    "c0 = (torch.ones(grad_x_r.size())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.4053, device='cuda:1', grad_fn=<AddBackward0>) 0 tensor(0.9655, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9995, device='cuda:1', grad_fn=<AddBackward0>) 1000 tensor(0.9991, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9995, device='cuda:1', grad_fn=<AddBackward0>) 2000 tensor(0.9990, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9993, device='cuda:1', grad_fn=<AddBackward0>) 3000 tensor(0.9989, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9986, device='cuda:1', grad_fn=<AddBackward0>) 4000 tensor(0.9981, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9943, device='cuda:1', grad_fn=<AddBackward0>) 5000 tensor(0.9909, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9920, device='cuda:1', grad_fn=<AddBackward0>) 6000 tensor(0.9852, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9910, device='cuda:1', grad_fn=<AddBackward0>) 7000 tensor(0.9829, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9878, device='cuda:1', grad_fn=<AddBackward0>) 8000 tensor(0.9773, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, device='cuda:1', grad_fn=<AddBackward0>) 9000 tensor(0.9741, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9865, device='cuda:1', grad_fn=<AddBackward0>) 10000 tensor(0.9734, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 11000 tensor(0.9732, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 12000 tensor(0.9731, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 13000 tensor(0.9731, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 14000 tensor(0.9731, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 15000 tensor(0.9730, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 16000 tensor(0.9730, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 17000 tensor(0.9730, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 18000 tensor(0.9730, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 19000 tensor(0.9730, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9864, device='cuda:1', grad_fn=<AddBackward0>) 20000 tensor(0.9730, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimzer = torch.optim.Adam(mynet.parameters(mynet),lr=0.0001)\n",
    "loss_func = Pinn_loss()\n",
    "epoch = 20000\n",
    "loss_all = np.zeros(epoch+1)\n",
    "for epoc in range(epoch+1):\n",
    "    out_r,out_i = mynet(x_in)[:,0],mynet(x_in)[:,1]\n",
    "    bd_out_r,bd_out_i = mynet(sample_x)[:,0],mynet(sample_x)[:,1]\n",
    "    \n",
    "    dx_r = torch.autograd.grad(out_r, x_in, grad_outputs=torch.ones(out_r.size()).to(device),retain_graph=True,create_graph=True)[0]\n",
    "    grad_x_r = (dx_r[:,0]).reshape(-1,1)\n",
    "    grad_y_r = (dx_r[:,1]).reshape(-1,1)\n",
    "    dxx_r = torch.autograd.grad(grad_x_r, x_in, grad_outputs=torch.ones(grad_x_r.size()).to(device),retain_graph=True,create_graph=True)[0]\n",
    "    dyy_r = torch.autograd.grad(grad_y_r, x_in, grad_outputs=torch.ones(grad_y_r.size()).to(device),retain_graph=True,create_graph=True)[0]\n",
    "    duu_r = (dxx_r[:,0]).reshape(-1,1)+(dyy_r[:,1]).reshape(-1,1)\n",
    "    \n",
    "    dx_i = torch.autograd.grad(out_i, x_in, grad_outputs=torch.ones(out_i.size()).to(device),retain_graph=True,create_graph=True)[0]\n",
    "    grad_x_i = (dx_i[:,0]).reshape(-1,1)\n",
    "    grad_y_i = (dx_i[:,1]).reshape(-1,1)\n",
    "    dxx_i = torch.autograd.grad(grad_x_i, x_in, grad_outputs=torch.ones(grad_x_i.size()).to(device),retain_graph=True,create_graph=True)[0]\n",
    "    dyy_i = torch.autograd.grad(grad_y_i, x_in, grad_outputs=torch.ones(grad_y_i.size()).to(device),retain_graph=True,create_graph=True)[0]\n",
    "    duu_i = (dxx_i[:,0]).reshape(-1,1)+(dyy_i[:,1]).reshape(-1,1)\n",
    "    \n",
    "    loss = loss_func(duu_r,duu_i,out_r,out_i,bd_out_r,bd_out_i,sample_u_r,sample_u_i,k_2)\n",
    "    optimzer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimzer.step()\n",
    "    if epoc%1000==0:\n",
    "        print(loss,epoc,torch.mean((bd_out_r-sample_u_r)**2+(sample_u_i-bd_out_i)**2))\n",
    "        \n",
    "    #loss_all[epoc]=loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9852, device='cuda:1', grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "#sample = 1000\n",
    "#theta_in = torch.rand(sample,1)*2*np.pi\n",
    "#r_in = torch.rand(sample,1).sqrt()*(9/20-1/9*torch.cos(5*theta_in))*0.99\n",
    "#x0_in = torch.cos(theta_in)\n",
    "#x1_in = torch.sin(theta_in)\n",
    "#x_in = (torch.cat((x0_in,x1_in),1)*r_in).to(device)\n",
    "out_r,out_i = mynet(x_in)[:,0],mynet(x_in)[:,1]\n",
    "u_in_r = (torch.cos(k1*x_in[:,0]+k2*x_in[:,1]))#.to(device)\n",
    "u_in_i = (torch.sin(k1*x_in[:,0]+k2*x_in[:,1]))#.to(device)\n",
    "#print(out_r)\n",
    "#print(u_in_r)\n",
    "#print(out_i)\n",
    "#print(u_in_i)\n",
    "print((((out_r-u_in_r)**2+(out_i-u_in_i)**2).sum()/((u_in_r)**2+(u_in_i)**2).sum()).sqrt())"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
